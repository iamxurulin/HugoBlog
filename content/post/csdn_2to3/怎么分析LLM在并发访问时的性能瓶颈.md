---
title: "怎么分析LLM在并发访问时的性能瓶颈"
date: 2026-01-13 15:28:15
categories:
- AI
tags:
  - 大模型
draft: false
---

试想一下这样一种**场景**：

如果一个GPU集群的LLM处理能力为1000 tokens/s，那么1000个用户同时并发访问的话，响应给每个用户的性能只有 1 token/s吗？

---
肯定不是。

因为LLM并不是简单的**线性分配资源**，而是通过**批处理**与**并发调度**的方式来提升吞吐量的。

LLM的核心计算是矩阵乘法，GPU的并行计算特性让“批量处理多个用户的tokens”耗时几乎不会增加，能充分地利用硬件资源。

如果每一次批处理包含100个用户请求，每个用户10个tokens，那么1000个用户可以分10批处理完，当用户的性能是**10 tokens/s**。

实际响应的速度取决于以下关键因素：

- **Token的长度**：**输入Token**影响批处理耗时，**输出Token**影响总响应时间，**流式输出**可以优化体感延迟；

- **批处理策略**：**静态批处理**简单并且易实现，**动态批处理**资源的利用率更高，连**续批处理**可以支撑超高并发；

- **资源排队机制**：FIFO、优先级队列等等策略决定**请求的等待时间**，不影响最终的处理速度。


